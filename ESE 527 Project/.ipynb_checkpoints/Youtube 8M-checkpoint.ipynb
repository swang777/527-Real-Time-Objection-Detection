{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11f749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bottleneck in c:\\users\\17240\\anaconda3\\lib\\site-packages (1.3.5)\n",
      "Collecting bottleneck\n",
      "  Obtaining dependency information for bottleneck from https://files.pythonhosted.org/packages/e1/ab/92c1292d7abcd424936f24470afc70a62601bd61bf95761832dfc88764da/Bottleneck-1.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading Bottleneck-1.4.1-cp311-cp311-win_amd64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\17240\\anaconda3\\lib\\site-packages (from bottleneck) (1.24.3)\n",
      "Downloading Bottleneck-1.4.1-cp311-cp311-win_amd64.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.6/111.6 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: bottleneck\n",
      "  Attempting uninstall: bottleneck\n",
      "    Found existing installation: Bottleneck 1.3.5\n",
      "    Uninstalling Bottleneck-1.3.5:\n",
      "      Successfully uninstalled Bottleneck-1.3.5\n",
      "Successfully installed bottleneck-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of train data... (Downloading shard=1,1000)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Function to run the download.py script with the appropriate parameters\n",
    "def download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards=''):\n",
    "    \"\"\"\n",
    "    Downloads YouTube-8M dataset shards using the local download.py script.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type (str): Can be 'train', 'validate', or 'test'.\n",
    "        partition (str): The partition path ('2/frame' for frame-level features).\n",
    "        mirror (str): The mirror to use for downloading (e.g., 'us', 'eu', or 'asia').\n",
    "        shards (str): Specifies the shard range to download (e.g., 'shard=1,1000').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set environment variables for partition and mirror\n",
    "    os.environ['partition'] = partition + '/' + dataset_type\n",
    "    os.environ['mirror'] = mirror\n",
    "    \n",
    "    # Adjust command to include the shard restriction (e.g., download the first 3 shards)\n",
    "    base_command = f'python download.py --partition={partition}/{dataset_type} --mirror={mirror} {shards}'\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starting download of {dataset_type} data... (Downloading {shards})\")\n",
    "        result = subprocess.run(base_command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the standard output and error\n",
    "        if result.stdout:\n",
    "            print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        if result.stderr:\n",
    "            print(f\"Standard Error:\\n{result.stderr}\")\n",
    "        \n",
    "        result.check_returncode()  # Check if the command was successful\n",
    "        print(f\"Successfully downloaded {dataset_type} data!\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while downloading {dataset_type} data: {e}\")\n",
    "        print(f\"Error Output: {e.stderr}\")\n",
    "\n",
    "# Set the directory where the download.py script is located\n",
    "data_dir = os.path.expanduser(\"~/data/yt8m/frame\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Download 3 shards (around 1.2 GB total) for training, validation, and test sets\n",
    "download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards='shard=1,1000')\n",
    "download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards='shard=2,1000')\n",
    "download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards='shard=3,1000')\n",
    "\n",
    "download_yt8m_shards(dataset_type='validate', partition='2/frame', mirror='us', shards='shard=1,1000')\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=1,1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of test data... (Downloading shard=1,1000)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Function to run the download.py script with the appropriate parameters\n",
    "def download_yt8m_shards(dataset_type='validate', partition='2/frame', mirror='us', shards=''):\n",
    "    \"\"\"\n",
    "    Downloads YouTube-8M dataset shards using the local download.py script.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type (str): Can be 'train', 'validate', or 'test'.\n",
    "        partition (str): The partition path ('2/frame' for frame-level features).\n",
    "        mirror (str): The mirror to use for downloading (e.g., 'us', 'eu', or 'asia').\n",
    "        shards (str): Specifies the shard range to download (e.g., 'shard=1,1000').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set environment variables for partition and mirror\n",
    "    os.environ['partition'] = partition + '/' + dataset_type\n",
    "    os.environ['mirror'] = mirror\n",
    "    \n",
    "    # Adjust command to include the shard restriction\n",
    "    base_command = f'python download.py --partition={partition}/{dataset_type} --mirror={mirror} {shards}'\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starting download of {dataset_type} data... (Downloading {shards})\")\n",
    "        result = subprocess.run(base_command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the standard output and error\n",
    "        if result.stdout:\n",
    "            print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        if result.stderr:\n",
    "            print(f\"Standard Error:\\n{result.stderr}\")\n",
    "        \n",
    "        result.check_returncode()  # Check if the command was successful\n",
    "        print(f\"Successfully downloaded {dataset_type} data!\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while downloading {dataset_type} data: {e}\")\n",
    "        print(f\"Error Output: {e.stderr}\")\n",
    "\n",
    "# Set the directory where the download.py script is located\n",
    "data_dir = os.path.expanduser(\"~/data/yt8m/frame\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Download 3 shards for validation data\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=1,1000')\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=2,1000')\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=3,1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11fa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define feature description for parsing TFRecord files\n",
    "feature_description = {\n",
    "    'video_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'labels': tf.io.VarLenFeature(tf.int64),  # Sparse, multi-labels\n",
    "    'mean_rgb': tf.io.FixedLenFeature([1024], tf.float32, default_value=[0.0]*1024),  # RGB features\n",
    "    'mean_audio': tf.io.FixedLenFeature([128], tf.float32, default_value=[0.0]*128)  # Audio features\n",
    "}\n",
    "\n",
    "# Parsing function for TFRecord files\n",
    "def _parse_function(proto):\n",
    "    return tf.io.parse_single_example(proto, feature_description)\n",
    "\n",
    "# Extract features and labels from the parsed record\n",
    "def extract_features_labels(record):\n",
    "    features = {\n",
    "        'mean_rgb': record['mean_rgb'],\n",
    "        'mean_audio': record['mean_audio']\n",
    "    }\n",
    "    # Convert sparse labels to dense, use -1 for missing labels\n",
    "    labels = tf.sparse.to_dense(record['labels'], default_value=-1)\n",
    "    return features, labels\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(file_pattern, batch_size=32, limit=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "    \n",
    "    :param file_pattern: Path pattern for the dataset (e.g., '*.tfrecord')\n",
    "    :param batch_size: Batch size for training\n",
    "    :param limit: Limit number of records for testing, if None all data is used\n",
    "    :return: TensorFlow dataset\n",
    "    \"\"\"\n",
    "    raw_dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(file_pattern))\n",
    "    parsed_dataset = raw_dataset.map(_parse_function)\n",
    "    dataset = parsed_dataset.map(extract_features_labels)\n",
    "    \n",
    "    if limit:\n",
    "        dataset = dataset.take(limit)  # Limit the dataset to a few records\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load datasets with a limit for faster experimentation\n",
    "train_dataset = load_dataset('~/data/yt8m/frame/train/*.tfrecord', batch_size=32, limit=5)\n",
    "validate_dataset = load_dataset('~/data/yt8m/frame/validate/*.tfrecord', batch_size=32, limit=5)\n",
    "test_dataset = load_dataset('~/data/yt8m/frame/test/*.tfrecord', batch_size=32, limit=5)\n",
    "\n",
    "# Inspect the first batch of training data\n",
    "for features, labels in train_dataset.take(1):\n",
    "    print(\"Mean RGB shape:\", features['mean_rgb'].shape)\n",
    "    print(\"Mean Audio shape:\", features['mean_audio'].shape)\n",
    "    print(\"Labels:\", labels.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d60fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
