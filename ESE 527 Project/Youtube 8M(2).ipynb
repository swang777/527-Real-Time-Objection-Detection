{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3edae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bottleneck in c:\\users\\17240\\anaconda3\\lib\\site-packages (1.3.5)\n",
      "Collecting bottleneck\n",
      "  Obtaining dependency information for bottleneck from https://files.pythonhosted.org/packages/e1/ab/92c1292d7abcd424936f24470afc70a62601bd61bf95761832dfc88764da/Bottleneck-1.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading Bottleneck-1.4.1-cp311-cp311-win_amd64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\17240\\anaconda3\\lib\\site-packages (from bottleneck) (1.24.3)\n",
      "Downloading Bottleneck-1.4.1-cp311-cp311-win_amd64.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.6/111.6 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: bottleneck\n",
      "  Attempting uninstall: bottleneck\n",
      "    Found existing installation: Bottleneck 1.3.5\n",
      "    Uninstalling Bottleneck-1.3.5:\n",
      "      Successfully uninstalled Bottleneck-1.3.5\n",
      "Successfully installed bottleneck-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of train data... (Downloading shard=1,1000)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Function to run the download.py script with the appropriate parameters\n",
    "def download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards=''):\n",
    "    \"\"\"\n",
    "    Downloads YouTube-8M dataset shards using the local download.py script.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type (str): Can be 'train', 'validate', or 'test'.\n",
    "        partition (str): The partition path ('2/frame' for frame-level features).\n",
    "        mirror (str): The mirror to use for downloading (e.g., 'us', 'eu', or 'asia').\n",
    "        shards (str): Specifies the shard range to download (e.g., 'shard=1,1000').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set environment variables for partition and mirror\n",
    "    os.environ['partition'] = partition + '/' + dataset_type\n",
    "    os.environ['mirror'] = mirror\n",
    "    \n",
    "    # Adjust command to include the shard restriction (e.g., download the first 3 shards)\n",
    "    base_command = f'python download.py --partition={partition}/{dataset_type} --mirror={mirror} {shards}'\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starting download of {dataset_type} data... (Downloading {shards})\")\n",
    "        result = subprocess.run(base_command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the standard output and error\n",
    "        if result.stdout:\n",
    "            print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        if result.stderr:\n",
    "            print(f\"Standard Error:\\n{result.stderr}\")\n",
    "        \n",
    "        result.check_returncode()  # Check if the command was successful\n",
    "        print(f\"Successfully downloaded {dataset_type} data!\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while downloading {dataset_type} data: {e}\")\n",
    "        print(f\"Error Output: {e.stderr}\")\n",
    "\n",
    "# Set the directory where the download.py script is located\n",
    "data_dir = os.path.expanduser(\"~/data/yt8m/frame\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Download 3 shards (around 1.2 GB total) for training, validation, and test sets\n",
    "download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards='shard=1,1000')\n",
    "download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards='shard=2,1000')\n",
    "download_yt8m_shards(dataset_type='train', partition='2/frame', mirror='us', shards='shard=3,1000')\n",
    "\n",
    "download_yt8m_shards(dataset_type='validate', partition='2/frame', mirror='us', shards='shard=1,1000')\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=1,1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbab9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of test data... (Downloading shard=1,1000)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Function to run the download.py script with the appropriate parameters\n",
    "def download_yt8m_shards(dataset_type='validate', partition='2/frame', mirror='us', shards=''):\n",
    "    \"\"\"\n",
    "    Downloads YouTube-8M dataset shards using the local download.py script.\n",
    "    \n",
    "    Args:\n",
    "        dataset_type (str): Can be 'train', 'validate', or 'test'.\n",
    "        partition (str): The partition path ('2/frame' for frame-level features).\n",
    "        mirror (str): The mirror to use for downloading (e.g., 'us', 'eu', or 'asia').\n",
    "        shards (str): Specifies the shard range to download (e.g., 'shard=1,1000').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set environment variables for partition and mirror\n",
    "    os.environ['partition'] = partition + '/' + dataset_type\n",
    "    os.environ['mirror'] = mirror\n",
    "    \n",
    "    # Adjust command to include the shard restriction\n",
    "    base_command = f'python download.py --partition={partition}/{dataset_type} --mirror={mirror} {shards}'\n",
    "    \n",
    "    try:\n",
    "        print(f\"Starting download of {dataset_type} data... (Downloading {shards})\")\n",
    "        result = subprocess.run(base_command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the standard output and error\n",
    "        if result.stdout:\n",
    "            print(f\"Standard Output:\\n{result.stdout}\")\n",
    "        if result.stderr:\n",
    "            print(f\"Standard Error:\\n{result.stderr}\")\n",
    "        \n",
    "        result.check_returncode()  # Check if the command was successful\n",
    "        print(f\"Successfully downloaded {dataset_type} data!\")\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error occurred while downloading {dataset_type} data: {e}\")\n",
    "        print(f\"Error Output: {e.stderr}\")\n",
    "\n",
    "# Set the directory where the download.py script is located\n",
    "data_dir = os.path.expanduser(\"~/data/yt8m/frame\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.chdir(data_dir)\n",
    "\n",
    "# Download 3 shards for validation data\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=1,1000')\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=2,1000')\n",
    "download_yt8m_shards(dataset_type='test', partition='2/frame', mirror='us', shards='shard=3,1000')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11fa9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: ~/data/yt8m/frame/train/*.tfrecord\n",
      "Parsing dataset...\n",
      "Parsing record...\n",
      "Parsed record: {'labels': SparseTensor(indices=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:0\", shape=(None, 1), dtype=int64), values=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:1\", shape=(None,), dtype=int64), dense_shape=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:2\", shape=(1,), dtype=int64)), 'mean_audio': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=(128,) dtype=float32>, 'mean_rgb': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=(1024,) dtype=float32>, 'video_id': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=() dtype=string>}\n",
      "Extracting features and labels from dataset...\n",
      "Extracted labels: Tensor(\"SparseToDense:0\", shape=(None,), dtype=int64)\n",
      "Loading dataset from: ~/data/yt8m/frame/validate/*.tfrecord\n",
      "Parsing dataset...\n",
      "Parsing record...\n",
      "Parsed record: {'labels': SparseTensor(indices=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:0\", shape=(None, 1), dtype=int64), values=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:1\", shape=(None,), dtype=int64), dense_shape=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:2\", shape=(1,), dtype=int64)), 'mean_audio': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=(128,) dtype=float32>, 'mean_rgb': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=(1024,) dtype=float32>, 'video_id': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=() dtype=string>}\n",
      "Extracting features and labels from dataset...\n",
      "Extracted labels: Tensor(\"SparseToDense:0\", shape=(None,), dtype=int64)\n",
      "Loading dataset from: ~/data/yt8m/frame/test/*.tfrecord\n",
      "Parsing dataset...\n",
      "Parsing record...\n",
      "Parsed record: {'labels': SparseTensor(indices=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:0\", shape=(None, 1), dtype=int64), values=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:1\", shape=(None,), dtype=int64), dense_shape=Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:2\", shape=(1,), dtype=int64)), 'mean_audio': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=(128,) dtype=float32>, 'mean_rgb': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=(1024,) dtype=float32>, 'video_id': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=() dtype=string>}\n",
      "Extracting features and labels from dataset...\n",
      "Extracted labels: Tensor(\"SparseToDense:0\", shape=(None,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define feature description for parsing TFRecord files\n",
    "feature_description = {\n",
    "    'video_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'labels': tf.io.VarLenFeature(tf.int64),  # Sparse, multi-labels\n",
    "    'mean_rgb': tf.io.FixedLenFeature([1024], tf.float32, default_value=[0.0]*1024),  # RGB features\n",
    "    'mean_audio': tf.io.FixedLenFeature([128], tf.float32, default_value=[0.0]*128)  # Audio features\n",
    "}\n",
    "\n",
    "# Parsing function for TFRecord files\n",
    "def _parse_function(proto):\n",
    "    print(\"Parsing record...\")\n",
    "    parsed_record = tf.io.parse_single_example(proto, feature_description)\n",
    "    print(f\"Parsed record: {parsed_record}\")\n",
    "    return parsed_record\n",
    "\n",
    "# Extract features and labels from the parsed record\n",
    "def extract_features_labels(record):\n",
    "    features = {\n",
    "        'mean_rgb': record['mean_rgb'],\n",
    "        'mean_audio': record['mean_audio']\n",
    "    }\n",
    "    # If the label is missing, you can set a default label or skip it\n",
    "    if 'labels' in record:\n",
    "        labels = tf.sparse.to_dense(record['labels'], default_value=-1)\n",
    "        print(f\"Extracted labels: {labels}\")\n",
    "    else:\n",
    "        labels = tf.constant([-1], dtype=tf.int64)  # Default label for missing cases\n",
    "        print(f\"Missing labels, assigning default: {labels}\")\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Function to load the dataset and limit to a few records for faster inspection\n",
    "def load_dataset(file_pattern, batch_size=32, limit=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset.\n",
    "    \n",
    "    :param file_pattern: Path pattern for the dataset (e.g., '*.tfrecord')\n",
    "    :param batch_size: Batch size for training\n",
    "    :param limit: Limit number of records for testing, if None all data is used\n",
    "    :return: TensorFlow dataset\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from: {file_pattern}\")\n",
    "    raw_dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(file_pattern))\n",
    "    \n",
    "    print(\"Parsing dataset...\")\n",
    "    parsed_dataset = raw_dataset.map(_parse_function)\n",
    "    \n",
    "    print(\"Extracting features and labels from dataset...\")\n",
    "    dataset = parsed_dataset.map(extract_features_labels)\n",
    "    \n",
    "    if limit:\n",
    "        dataset = dataset.take(limit)  # Limit the dataset to a few records\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Load datasets with a limit for faster experimentation\n",
    "train_dataset = load_dataset('~/data/yt8m/frame/train/*.tfrecord', batch_size=32, limit=5)\n",
    "validate_dataset = load_dataset('~/data/yt8m/frame/validate/*.tfrecord', batch_size=32, limit=5)\n",
    "test_dataset = load_dataset('~/data/yt8m/frame/test/*.tfrecord', batch_size=32, limit=5)\n",
    "\n",
    "# Inspect the first batch of training data\n",
    "for features, labels in train_dataset.take(1):\n",
    "    print(\"Mean RGB shape:\", features['mean_rgb'].shape)\n",
    "    print(\"Mean Audio shape:\", features['mean_audio'].shape)\n",
    "    print(\"Labels:\", labels.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4d60fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 204: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading record \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Path to the shard you want to inspect\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m inspect_shard_raw(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/data/yt8m/frame/train/train0082.tfrecord\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36minspect_shard_raw\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minspect_shard_raw\u001b[39m(file_path):\n\u001b[0;32m      5\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTFRecordDataset(file_path)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, raw_record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Record \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_record\u001b[38;5;241m.\u001b[39mnumpy()[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print first 100 bytes of each record\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_internal()\n\u001b[0;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39miterator_get_next(\n\u001b[0;32m    777\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[0;32m    778\u001b[0m       output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    779\u001b[0m       output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes)\n\u001b[0;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3108\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3107\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3108\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   3109\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteratorGetNext\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, iterator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_types\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_types,\n\u001b[0;32m   3110\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_shapes)\n\u001b[0;32m   3111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3112\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 204: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Function to read raw binary records from the TFRecord file\n",
    "def inspect_shard_raw(file_path):\n",
    "    dataset = tf.data.TFRecordDataset(file_path)\n",
    "    for idx, raw_record in enumerate(dataset):\n",
    "        try:\n",
    "            print(f\"Raw Record {idx}: {raw_record.numpy()[:100]}...\")  # Print first 100 bytes of each record\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading record {idx}, skipping: {e}\")\n",
    "\n",
    "# Path to the shard you want to inspect\n",
    "inspect_shard_raw(\"~/data/yt8m/frame/train/train0082.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed2989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
